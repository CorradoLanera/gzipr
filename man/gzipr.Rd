% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gzipr.R
\name{gzipr}
\alias{gzipr}
\alias{gzipr.character}
\alias{gzipr.data.frame}
\alias{gzipr.list}
\alias{gzipr.gzipr}
\alias{gzipr.default}
\title{gzipr}
\usage{
gzipr(x, y = NULL)

\method{gzipr}{character}(x, y = NULL)

\method{gzipr}{data.frame}(x, y = NULL)

\method{gzipr}{list}(x, y = NULL)

\method{gzipr}{gzipr}(x, y = NULL)

\method{gzipr}{default}(x, y = NULL)
}
\arguments{
\item{x}{training data}

\item{y}{(default = \code{NULL}) labels for training data, must be the
same length of \code{x}, i.e., one label per observation. If provided,
\code{y} is used as labels for \code{x}, otherwise, if \code{x} is named
(\code{rownames} for \code{data.frame}s, \code{names} for \code{character}s) \code{y} is set
to the names of \code{x}. If \code{x} is not named, \code{y} must be provided.}
}
\value{
a \code{gzipr} model object
}
\description{
Citing the original paper: Deep neural networks (DNNs) are often used
for text classification tasks as they usually achieve high levels of
accuracy. However, DNNs can be computationally intensive with
billions of parameters and large amounts of labeled data, which can
make them expensive to use, to optimize and to transfer to
out-of-distribution (OOD) cases in practice. In this paper, we
propose a non-parametric alternative to DNNs that's easy,
light-weight and universal in text classification: a combination of a
simple compressor like gzip with a k-nearest-neighbor classifier.
Without any training, pre-training or fine-tuning, our method
achieves results that are competitive with non-pretrained deep
learning methods on six in-distributed datasets. It even outperforms
BERT on all five OOD datasets, including four low-resource languages.
Our method also performs particularly well in few-shot settings where
labeled data are too scarce for DNNs to achieve a satisfying
accuracy.
}
\section{Methods (by class)}{
\itemize{
\item \code{gzipr(character)}: method to train on \code{character} vectors as input
data

\item \code{gzipr(data.frame)}: method to train on \code{data.frame}s as input data

\item \code{gzipr(list)}: \ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#experimental}{\figure{lifecycle-experimental.svg}{options: alt='[Experimental]'}}}{\strong{[Experimental]}}
method to train on \code{list}s as input data

\item \code{gzipr(gzipr)}: method to retrain a \code{gzipr} model maintaining the
same data and possibly changing the labels

\item \code{gzipr(default)}: catch-all method for not yet implemented training
objects

}}
\examples{
library(gzipr)

train <- mtcars[1:20, ]
test <- mtcars[21:32, ]

train_x <- train[, -2]
train_y <- train[[2]]

test_x <- test[, -2]
test_y <- test[[2]]

model <- gzipr(train_x, train_y)
result <- predict(model, test_x)

# accuracy
mean(result == test_y)
}
\references{
Jiang, Zhiying, Matthew Y. R. Yang, Mikhail Tsirlin,
Raphael Tang, and Jimmy Lin. 2022. "Less Is More: Parameter-Free
Text Classification with Gzip." - http://arxiv.org/abs/2212.09410
}
